{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c54e9f03-7f34-4c6e-b75a-fa4a925b1ec5",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "Also called __Hierarchical Cluster Analysis__ or __HCA__ is an unsupervised clustering algorithm which involves creating clusters that have __predominant ordering__ from top to bottom.\n",
    "\n",
    "For e.g: All files and folders on our hard disk are organized in a hierarchy.\n",
    "\n",
    "The algorithm groups similar objects into groups called clusters. The endpoint is a set of clusters or groups, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each otlustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639fb688-4d3b-455b-aaa5-37da930d0aa9",
   "metadata": {},
   "source": [
    "![image.png](https://cdn-images-1.medium.com/max/1600/1*ET8kCcPpr893vNZFs8j4xg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de2370-1198-40e2-9ea6-86ed6e52f6c9",
   "metadata": {},
   "source": [
    "This clustering technique is divided into two types:\n",
    "\n",
    "* __Agglomerative Hierarchical Clustering__\n",
    "* __Divisive Hierarchical Clustering__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a38572-1880-4a6e-bccd-3152087cad4b",
   "metadata": {},
   "source": [
    "## 1. Agglomerative Hierarchical Clustering\n",
    "\n",
    "The Agglomerative Hierarchical Clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity. It’s also known as **AGNES** (Agglomerative Nesting). \n",
    "\n",
    "It's a _“bottom-up”_ approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.tanding:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e636f7c-0179-4f6d-bc6a-6ee8ab5e9673",
   "metadata": {},
   "source": [
    "__How does it work?__\n",
    "\n",
    "* Make each data point a single-point cluster → forms N clusters\n",
    "* Take the two closest data points and make them one cluster → forms N-1 clusters\n",
    "* Take the two closest clusters and make them one cluster → Forms N-2 clusters.\n",
    "* Repeat step-3 until you are left with only one cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb08da-2139-461a-a388-3c1b3af65b10",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/257/0*iozEcRXXWXbDMrdG.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9289be5-0268-43c7-99ec-c71af5417248",
   "metadata": {},
   "source": [
    "Another example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb40073-30ed-4ee7-bfc6-fe337268e8d4",
   "metadata": {},
   "source": [
    "<img src=\"https://images.datacamp.com/image/upload/v1674149819/Dendrogram_of_Agglomerative_Clustering_Approach_4eba3586ec.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd0c50-3da9-41ec-8770-58efd6bd5216",
   "metadata": {},
   "source": [
    "As showing in the illustration above: \n",
    "\n",
    "* We start by considering each animal to be its unique cluster.  \n",
    "* Then we generate three different clusters from those unique animals based on their similarities: \n",
    "    * __Birds__: Eagle and Peacock\n",
    "    * __Mammals__:  Lion and Bear\n",
    "    * __More than three leg animals__: Spider and Scorpion.\n",
    "* We repeat the merging process to create the vertebrate cluster by combining the two most similar clusters: __Birds__ and __Mammals__.\n",
    "\n",
    "After this step, the remaining two clusters, __Vertebrate__ and __More than three legs__, are merged to create a single __Animals__ cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2154842b-62a2-4712-9de6-02ce8337ef10",
   "metadata": {},
   "source": [
    "## 2. Divisive Hierarchical Clustering\n",
    "\n",
    "In Divisive or DIANA(DIvisive ANAlysis Clustering) is a _top-down_ clustering method where we assign all of the observations to a single cluster and then partition the cluster to two least similar clusters. Finally, we proceed recursively on each cluster until there is one cluster for each observation. So this clustering approach is exactly opposite to Agglomerative clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33cc03-f062-4b3c-824b-18a4154777bd",
   "metadata": {},
   "source": [
    "<img src=\"https://images.datacamp.com/image/upload/v1674149823/Dendrogram_of_Divisive_Clustering_Approach_8623354c7b.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af597667-9359-41ee-9c30-f28e3c5d318c",
   "metadata": {},
   "source": [
    "From this divisive approach on the image above: \n",
    "ls. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6df14-fb40-4c61-a6e2-e7143090c44f",
   "metadata": {},
   "source": [
    "* We notice that the whole animal dataset is considered as a single block.\n",
    "* Then, we divide that block into two clusters: Vertebrate and More than 3 legs. \n",
    "\n",
    "The division process is iteratively applied to the previously created clusters until we get unique animals. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4f8b54-0d8e-467b-8d7d-5964d3ed7a22",
   "metadata": {},
   "source": [
    "## Distance measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec5aedf-cee5-4ada-a3fb-d65c2577ecf1",
   "metadata": {},
   "source": [
    "Your choice of distance measure is a critical step in clustering, and it depends on the problem you’re trying to solve. \n",
    "\n",
    "Considering the following scenario, we could cluster students based on any number of approaches such as their: \n",
    "\n",
    "* Country of origin\n",
    "* Gender, either male or female\n",
    "* Previous academic background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89150067-c809-4c8b-8e3c-7ef4cbc9ac26",
   "metadata": {},
   "source": [
    "These are all valid clusters but differ in meaning. \n",
    "\n",
    "Even though __Euclidean distance__ is the most common distance used in most clustering software, there are other distance measures such as __Manhattan distance__, __Canberra distance__, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f09e3bf-9c59-4416-83b9-643b94ff02db",
   "metadata": {},
   "source": [
    "### Linkage Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f9ae2-089f-46a6-987b-b83e7219484c",
   "metadata": {},
   "source": [
    "There are several ways to measure the distance between clusters in order to decide the rules for clustering, and they are often called __Linkage Methods__. Some of the common linkage methods are:\n",
    "\n",
    "#### 1. Single-linkage\n",
    "\n",
    "The distance between two clusters is defined as the shortest distance between two points in each cluster. This linkage may be used to detect high values in your dataset which may be outliers as they will be merged at the end\n",
    "\n",
    "<img src=\"https://images.datacamp.com/image/upload/v1674149819/Single_linkage_illustration_ea623e18a4.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c4b52-6c57-47e7-8362-649eac8f72ca",
   "metadata": {},
   "source": [
    "#### 2. Complete-linkage\n",
    "\n",
    "The distance between two clusters is defined as the longest distance between two points in each cluster.\n",
    "\n",
    "<img src=\"https://images.datacamp.com/image/upload/v1674149821/Complete_linkage_illustration_982fb26b4c.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f1ea6-af06-4b25-8144-6820e9a26295",
   "metadata": {},
   "source": [
    "#### 3. Average-linkage\n",
    "\n",
    "The distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster.\n",
    "\n",
    "<img src=\"https://images.datacamp.com/image/upload/v1674149821/Average_linkage_illustration_edeec7a09e.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443fc704-34d6-4c17-9e92-6746c8f6d63b",
   "metadata": {},
   "source": [
    "#### 4. Ward's linkage / method\n",
    "\n",
    "Ward's linkage aims to minimize the variance within the merged clusters: merging clusters should minimize the overall increase in variance after merging. This leads to more compact and well-separated clusters.\n",
    "\n",
    "<img src=\"https://s3.stackabuse.com/media/articles/definitive-guide-to-hierarchical-clustering-with-python-and-scikit-learn-15.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af09bf3d-d492-4118-85fc-80a142e539a5",
   "metadata": {},
   "source": [
    "The choice of linkage method entirely depends on you and there is no hard and fast method that will always give you good results. Different linkage methods lead to different clusters.\n",
    "\n",
    "The point of doing all this is to demonstrate the way hierarchical clustering works, it maintains a memory of how we went through this process and that memory is stored in **Dendrogram**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bf7cb4-0ffa-4351-a379-2bcb6ab88fb3",
   "metadata": {},
   "source": [
    "## What is a Dendogram?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ce73a0-e560-47da-ac11-f151b515a760",
   "metadata": {},
   "source": [
    "A Dendrogram is a type of tree diagram showing __hierarchical relationships__ between different sets of data.\n",
    "\n",
    "As already said a Dendrogram contains the __memory of hierarchical clustering algorithm__, so just by looking at the Dendrgram you can tell how the cluster is formed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a205a4f-b6c3-4f5f-9188-5aabaeaf6ac7",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/480/0*BfO2YN_BSxThfUoo.gif\" width=\"600\">d.m."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b16a2-c3d5-4bce-a82f-b38a553ae6c0",
   "metadata": {},
   "source": [
    "In the hierarchical tree structure, the **leaves** denote the instances or the data points in the data set. \n",
    "\n",
    "The corresponding distances at which the merging or grouping occurs can be inferred from the y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e81fe09-b805-499d-b955-1d5c4e796129",
   "metadata": {},
   "source": [
    "Because the type of __linkage__ determines how the data points are grouped together, different linkage criteria yield different dendrograms.\n",
    "\n",
    "Based on the distance, we can use the __dendrogram cut or slice__ it at a specific point—to get the required number of clusters.s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f390a-c6a2-4f58-97a7-ea24cb3c9efb",
   "metadata": {},
   "source": [
    "You cut the dendrogram tree with a horizontal line at a height where the line can __traverse the maximum distance up and down without intersecting the merging point__.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/314/0*lO30pyuAmDk6h_0T.jpg\" width=\"600\">\n",
    "\n",
    "For example in the above figure, __L3__ can traverse maximum distance up and down without intersecting the merging points. So we draw a horizontal line and the number of verticle lines it intersects is the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c0c7d-1fb3-4029-826e-edaecc111892",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Unlike some clustering algorithms like K-Means clustering, hierarchical clustering does not require you to specify the number of clusters beforehand. However, agglomerative clustering can be __computationally very expensive__ when working with large datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d07d0-d04d-4a25-8fe5-5f8c7df53a72",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf77eb-e91b-4a0a-b25a-046adcaf4937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4478319b-faec-4b5a-9fe1-0914a8b01685",
   "metadata": {},
   "source": [
    "Next, we load the wine dataset into a pandas dataframe. It is a simple dataset that is part of scikit-learn’s datasets and is helpful in exploring hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612944f2-0ab3-49b6-8900-276f02097295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "\n",
    "# Convert to DataFrame\n",
    "wine_df = pd.DataFrame(X, columns=data.feature_names)\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e11ece-222f-4808-ba0b-2ec923bfd6b6",
   "metadata": {},
   "source": [
    "Because the data set contains numeric values that are spread across different ranges, let's preprocess the dataset. We’ll use MinMaxScaler to transform each of the features to take on values in the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe6ead-d7b4-4d79-95ac-e4e5dc5d8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2571c7-a3df-4f99-9abe-350f15ab47e6",
   "metadata": {},
   "source": [
    "Let’s compute the linkage matrix, perform clustering, and plot the dendrogram. We can use linkage from the hierarchy module to calculate the linkage matrix based on __Ward's linkage__ (set method to ``ward``).\n",
    "\n",
    "As discussed, __Ward's linkage__ minimizes the _variance_ within each cluster. \n",
    "\n",
    "We then plot the dendrogram to visualize the hierarchical clustering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd55fd-b585-4d97-a68e-3bb0f31ac37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate linkage matrix\n",
    "linked = linkage(X_scaled, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(8, 4))\n",
    "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b034ae-60d5-4721-8c41-f46d66a71f2e",
   "metadata": {},
   "source": [
    "#### Truncating the Dendrogram for Easier Visualization\n",
    "\n",
    "In practice, instead of the entire dendrogram, we can visualize a truncated version that's easier to interpret and understand.\n",
    "\n",
    "To truncate the dendrogram, we can set truncate_mode to ``level`` and $p = 3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0398904b-9ac3-4a43-88fd-00ce5ad636e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate linkage matrix\n",
    "linked = linkage(X_scaled, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(8, 4))\n",
    "dendrogram(linked, orientation='top', distance_sort='descending', truncate_mode='level', p=3, show_leaf_counts=True)\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Distance')\n",
    "plt.axhline(y=4, color = 'black', linestyle = '--') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8d0b68-0f28-4658-9f4c-43daecbc3a84",
   "metadata": {},
   "source": [
    "The dendrogram helps us choose the __optimal number of clusters__. \n",
    "\n",
    "We can observe where the distance along the y-axis increases drastically, choose to truncate the dendrogram at that point, and use the distance as the threshold to form clusters. \n",
    "\n",
    "For this example, the optimal number of clusters is $3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1743242e-0612-434b-bfe4-374ac84741d2",
   "metadata": {},
   "source": [
    "#### Visualize the Clusters\n",
    "\n",
    "Now that each data point has been assigned to a cluster, you can visualize a subset of features and their cluster assignments. Here's the scatter plot of two such features along with their cluster mapping: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a1bbb-4277-43c8-adba-eedce14289ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "clustering_model = clustering.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf0eed-efbb-4d95-8a0b-cdc4fc1249ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pd.DataFrame(X_scaled, columns=wine_df.columns)\n",
    "X_new['cluster'] = clustering_model.astype(str)\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea6f60-25db-4ba5-a331-6d064e83a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter(X_new, x=X_new['alcohol'], y=X_new[\"flavanoids\"], color=\"cluster\", width=800, height=600)\n",
    "fig.update_layout(\n",
    "    title=\"Hierarchical Clustering\",\n",
    "    xaxis_title=\"Alcohol\",\n",
    "    yaxis_title=\"Flavanoids\",\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
