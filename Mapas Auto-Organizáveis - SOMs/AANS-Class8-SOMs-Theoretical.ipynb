{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb42a1af-ed98-4af8-b3a6-1a6421ee8f74",
   "metadata": {},
   "source": [
    "# Self-Organizing Maps (SOMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f59d1c-8f65-462a-a0c7-44aea29b47a7",
   "metadata": {},
   "source": [
    "A **Self-Organizing Map** (**SOM**), also known as a Kohonen map, is a type of [artificial neural network](https://en.wikipedia.org/wiki/Artificial_neural_network) (ANN) that is utilized for __unsupervised learning__. Unlike many traditional neural networks that are trained through supervised learning with labeled datasets, SOMs __organize data autonomously__. They produce a low-dimensional, typically two-dimensional, discretized representation of the input space of the training samples. This representation is referred to as a **map** and is a form of dimensionality reduction.\n",
    "\n",
    "SOMs distinguish themselves from other types of ANNs by employing **competitive learning** rather than **error-correction learning**, which is used in methods such as **backpropagation with gradient descent** (e.g., stochastic gradient descent or SGD). Competitive learning involves neurons competing for activation, whereby the neuron most similar to the input vector is __activated__ (often called the __Best Matching Unit__ or __BMU__), while nearby neurons on the map are also updated to make them more similar to the input, enhancing the preservation of the topological properties of the input space.\n",
    "\n",
    "![Self-Organizing Map](https://ivape3.blogs.uv.es/files/2015/03/kohonen1.png)\n",
    "\n",
    "SOMs feature only **two layers**: an **input layer** and an **output layer**, also known as the feature map. The input layer directly connects to the output layer through a set of adjustable weights without any intermediate layers. Uniquely, the SOM’s neurons in the output layer do not apply an activation function as typically seen in other neural network architectures. Instead, the training process involves adjusting the weights directly based on the input data, with the aim to minimize the difference between the input vector and the neuron's weight vector.\n",
    "\n",
    "The feature map of a SOM is organized such that __spatially close nodes in the map represent input data that are similar__, thereby preserving the **topological properties** of the input space. This characteristic makes SOMs particularly useful for visualizing high-dimensional data in a way that highlights intrinsic similarities among data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5eb416-51cb-488c-a1a2-da247ff39e06",
   "metadata": {},
   "source": [
    "## How SOM works?\n",
    "\n",
    "**SOM** was introduced by Finnish professor Teuvo Kohonen in the 1980s is sometimes called a **Kohonen map.**\n",
    "\n",
    "**Referece**: [_Applications of the growing self-organizing map, Th. Villmann, H.-U. Bauer, May 1998_](https://www.sciencedirect.com/science/article/abs/pii/S092523129800037X)\n",
    "\n",
    "Short explanation video here: https://www.youtube.com/watch?v=5CvJWtxytIk\n",
    "\n",
    "A SOM is a grid of neurons. Each neuron has $N$ weights, where $N$ is the __number of dimensions (features)__ in the data.\n",
    "\n",
    "The training process can be summarized in seven steps:\n",
    "\n",
    "1.  Assign **random weights** to the neurons.\n",
    "2.  Select an **input data** point.\n",
    "3.  Calculate the distance between the data point and each neuron.\n",
    "4.  Identify the neuron with the __smallest distance__ to the data point.\n",
    "\n",
    "This neuron is referred to as the **Best Matching Unit (BMU)** or winning neuron. A small distance means that the neuron is very close to the data point.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/272293965/figure/fig2/AS:613955332476961@1523389766981/Self-organizing-map-learning-process-The-2-dimensional-grid-of-neurons-is-characterized.png\" width=600>\n",
    "\n",
    "**Best Matching Unit** is a technique which calculates the distance from each weight to the sample vector, by running through all weight vectors. The weight with the shortest distance is the winner. There are numerous ways to determine the distance, however, the most commonly used method is the [_Euclidean Distance,_](https://en.wikipedia.org/wiki/Euclidean_distance) _and that’s what is used in the following implementation._\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:700/0*UAOUIrS7Gtl3D_U6.png\" width=600>\n",
    "\n",
    "5. Drag the **winning neuron closer** to the data point.\n",
    "\n",
    "When you drag a neuron, its neighbors (in the radius) are also dragged. The closer the neuron, the more drastically it is dragged, resulting in a larger change in its weight vector.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:700/0*hRthmx-SmO3vYmvT.png)\n",
    "\n",
    "6. Decrease the radius **gradually**, so that neurons **farther away** from the BMU are **less affected** by the updates.\n",
    "\n",
    "Fig. 1 | Fig. 2\n",
    "-|-\n",
    "<img src=\"https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/62_blog_image_1.png\" width=400> | <img src=\"https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/62_blog_image_2.png\" width=400>\n",
    "\n",
    "_**A radius that is too small could lead to overfitting while a bigger one could underfit the data!**_\n",
    "\n",
    "7. **Repeat steps 2 to 7**\n",
    "\n",
    "In the resulting map, each winning neuron is assigned to a particular cluster based on its location and the distribution of the input data. Neurons that are close to each other in the SOM are likely to be assigned to the same cluster, reflecting similarities in the underlying data patterns.\n",
    "\n",
    "Fig. 1 | Fig. 2\n",
    "-|-\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:547/0*yud5epzUGK6NvA6F.png\" width=400> | <img src=\"https://qph.cf2.quoracdn.net/main-qimg-b44196ae753416d9d95d4a89d7b0da39\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb607f1-7f6a-4a02-9d0a-fc8d2f1490ba",
   "metadata": {},
   "source": [
    "## Competitive Learning: What does it mean?\n",
    "\n",
    "As we mention before, SOM doesn’t use backpropagation with SGD to update weights, this type of unsupervised artificial neural network uses competetive learning to update its weights.\n",
    "\n",
    "__Competetive learning__ is based on these three processes:\n",
    "\n",
    "- **Competetion**\n",
    "- **Cooperation**\n",
    "- **Adaptation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81191d72-411c-47fa-9051-420b4f32103f",
   "metadata": {},
   "source": [
    "### 1. Competetion\n",
    "\n",
    "Like it was said before, each neuron in a SOM is assigned a **weight vector** with the same dimensionality as the input space.\n",
    "\n",
    "In the example below, in each neuron of the output layer we will have a vector with dimension $N$.\n",
    "\n",
    "We compute distance between each neuron (neuron from the output layer) and the input data, and the neuron with the lowest distance will be the winner of the competetion. \n",
    "\n",
    "This stage is called competition because the neurons compete in similarity to our data point.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:700/1*Gf70S1DYJyhq_mVzXr3G7Q.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f4e09-0c02-4983-9f02-a6f4c822b25c",
   "metadata": {},
   "source": [
    "### 2. Cooperation\n",
    "\n",
    "**The cooperation phase begins after finding the winning neuron.** In it, we find the winning neuron’s neighbors.\n",
    "\n",
    "The neighborhood is defined through the neighborhood function ![h](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-2ce27f7d2d82e3b238176ec7e7ee9118_l3.svg \"Rendered by QuickLaTeX.com\"). It quantifies the degree to which a neuron can be considered a neighbor of the winning neuron. While doing so, ![h](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-2ce27f7d2d82e3b238176ec7e7ee9118_l3.svg \"Rendered by QuickLaTeX.com\") follows two rules:\n",
    "\n",
    "- it should be symmetrical about the winning neuron\n",
    "- its value should decrease with the distance to the neuron\n",
    "\n",
    "The image below show us how the winner neuron’s ( The most green one in the center) neighbors are choosen depending on distance and time factors.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:609/1*OBoUEzgu1Y-xm_sQW1GoqQ.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15a56b2-1cac-4b54-b1fb-e6461281d2f8",
   "metadata": {},
   "source": [
    "### 3. Adaptation\n",
    "\n",
    "After choosing the winner neuron and it’s neighbors we compute neurons update. Those choosen neurons will be updated but not the same update, more the distance between neuron and the input data grow less we adjust it like shown in the image below :\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:581/1*5H8fuicZ2ABNdxgQ7GR9mw.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e224ed-c9d3-43e5-bb0c-528f802605e7",
   "metadata": {},
   "source": [
    "#### X. Resumed GIF\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/37/StepTrainingSOM.gif\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c175e520-dc47-47b4-83ef-d9ee3692c973",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "We can use SOMs for:\n",
    "\n",
    "-   Clustering: grouping similar entities.\n",
    "-   Data Visualization: visualizing high dimensional space in 2D.\n",
    "-   Anomaly detection: identifying uncommon behavior.\n",
    "-   etc.\n",
    "\n",
    "More concretely, we can use SOMs to:\n",
    "\n",
    "-   Build recommender system: grouping similar movies, music, product reviews, etc.\n",
    "-   Identify trends and patterns: find similar employees, customers, social media topics, stock sector performances, etc.\n",
    "-   Detect outliers: Detect and prevent fraud, security breaches, and other abnormal behavior.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "SOMs have some limitations, such as:\n",
    "\n",
    "-   Poor performance with categorical data.\n",
    "-   Lack of interpretability.\n",
    "-   Limited ability to handle high-dimensional data.\n",
    "-   Limited generalization ability: not well-suited for unseen data.\n",
    "-   Difficulty with non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e61467-a4d9-48f0-af35-39db264c5f14",
   "metadata": {},
   "source": [
    "## SOMs vs. K-means\n",
    "\n",
    "**SOMs and K-means can both be used for clustering the data.** However, we can think of SOMs as a constrained version of K-means. This means that the neurons in SOMs aren’t as free to move as the centroids in K-means are.\n",
    "\n",
    "**However, in K-means, we have to specify the number of clusters beforehand, while SOMs don’t need that.** The reason is that the initial lattice that we choose can be large, allowing neurons to cluster on their own. We can see this in a [diagram](https://commons.wikimedia.org/wiki/File:2D_data_training_SOM.gif) of how SOMs work:\n",
    "\n",
    "<img src=\"https://www.baeldung.com/wp-content/uploads/sites/4/2023/05/2D_data_training_SOM-1.gif\" width=400>\n",
    "\n",
    "The nearby neurons move together and reveal clusters in the data. We don’t need to specify the number of clusters beforehand since the neurons will learn it from the data.\n",
    "\n",
    "Another drawback of K-means is that it can be more sensitive to outliers since they can move the centroids toward themselves, away from the clusters in the data. This is not the case with SOMs.\n",
    "\n",
    "**Finally, SOMs keep the topological structure of the data, whereas K-means doesn’t.** This is because, in K-means, each centroid moves freely without regard for the others. However, in SOMs, the shift in a neuron’s place is relative to the winning neuron. As a result, SOMs keep the underlying topology of the data, which is shown in the above diagram."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
