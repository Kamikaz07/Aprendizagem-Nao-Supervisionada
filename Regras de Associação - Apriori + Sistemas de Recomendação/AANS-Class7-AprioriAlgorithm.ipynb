{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824ef763-c416-4f89-9704-cafc07b0c18a",
   "metadata": {},
   "source": [
    "# Association Rules - Market Basket Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234914c-b1de-40e9-9ab1-97d556da39e9",
   "metadata": {},
   "source": [
    "Some of us go to the grocery with a standard list; while some of us have a hard time sticking to our grocery shopping list, no matter how determined we are. No matter which type of person you are, retailers will always be experts at making various temptations to inflate your budget.\n",
    "\n",
    "Remember the time when you had the “_Ohh, I might need this as well._” moment? Retailers boost their sales by relying on this one simple intuition.\n",
    "\n",
    "> **_People that buy this will most likely want to buy that as well._**\n",
    "\n",
    "People who buy bread will have a higher chance of buying butter together, therefore an experienced assortment manager will definitely know that having a discount on bread pushes the sales on butter as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657d615-0319-4469-be3d-71a2b4d6baed",
   "metadata": {},
   "source": [
    "## Data-driven strategies\n",
    "\n",
    "Huge retailers pivot on a detailed market basket analysis to uncover associations between items. Using this valuable information, they are able to carry out various strategies to improve their revenue:\n",
    "\n",
    "- Associated products are placed close to each other, so that buyers of one item would be prompted to buy the other.\n",
    "- Discounts can be applied to only one of the associated products.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:770/0*EKYQHLhOlfpW3FeW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e61d1-89f9-452d-bcf0-0ea393b52932",
   "metadata": {},
   "source": [
    "## Association Rule Mining\n",
    "\n",
    "But how exactly is a Market Basket Analysis carried out?\n",
    "\n",
    "Data scientists are able to carry out Market Basket Analysis by implementing Association Rule Mining. Association Rule Mining is a **rule-based machine learning method that helps to uncover meaningful correlations between different products** according to their co-occurrence in a data set.\n",
    "\n",
    "However, one of the major pitfalls is that it consists of various formulas and parameters that may make it difficult for people without expertise in data mining. Therefore, before sharing your results with stakeholders, make sure that the underlying definitions are well-understood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4628fe1c-09f9-4d84-80dc-25d41bee9127",
   "metadata": {},
   "source": [
    "## Core concepts illustration\n",
    "\n",
    "I will be illustrating three of the core concepts that are used in Association Rule Mining with some simple examples below. This will assist you in grasping the data mining process.\n",
    "\n",
    "Let’s say you have now opened up your own cafeteria. How will you utilize your data science skills to __understand which of the items on your menu are associated__?\n",
    "\n",
    "There are six transactions in total with various different purchases that happened in your cafeteria.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:201/1*ak7rukRbBei1LAD-jHNuig.png)\n",
    "\n",
    "We can utilize three core measures that are used in Association Rule Learning, which are: **Support**, **Confidence,** and **Lift**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d1d462-fe8b-45ce-b1ef-d4754226c23d",
   "metadata": {},
   "source": [
    "### 1. Support\n",
    "\n",
    "**Support is just the plain basic probability of an event to occur.** It is measured by the __proportion of transactions in which an item set appears__. \n",
    "\n",
    "To put it in another way, $Support(A)$ is the number of transactions which includes A divided by the total number of transactions.\n",
    "\n",
    "If we analyze the transaction table above, the support for ``cookie`` is $3$ out of $6$. That is, out of a total of __$6$ transactions__, purchases containing ``cookies`` have occurred __3 times__ (or $50\\%$).\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:184/1*JvwE_0DeslBzi9HJTYLVrw.png)\n",
    "\n",
    "Support can be implemented onto multiple items at the same time as well. The support for ``cookie`` and ``cake`` is 2 out of 6.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:267/1*zfzXNNkUTPVFOgLkSvnNwQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888f5084-6437-4188-a2c2-94dffaacd8d3",
   "metadata": {},
   "source": [
    "### 2. Confidence\n",
    "\n",
    "The confidence of a **consequent** **event** given an **antecedent event** can be described by using __conditional probability__ (_Bayes' Theorem_). \n",
    "\n",
    "Simply put, it is **the probability of event $A$ happening given that event $B$ has already happened.**\n",
    "\n",
    "This can be used to describe the probability of an item being purchased when another item is already in the basket. It is measured by dividing the proportion of transactions with item $X$ and $Y$, over the proportion of transactions with $Y$.\n",
    "\n",
    "From the transactions table above, the confidence of ``{cookie -> cake}`` can be formulated below:\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:560/1*faL2TJQAWLdKToSqp9LJkQ.png)\n",
    "\n",
    "The __conditional probability__ can also be written as:\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:477/1*-ecvkPBR_2sVPsBJFYx2lw.png)\n",
    "\n",
    "Finally, we arrive at a solution of $2$ out of $3$. We can understand the intuition of confidence if we were to look only at ``Transaction 1`` to ``Transaction 3``. Out of $3$ purchases with ``cookies``, $2$ of them are actually bought together with a ``cake``!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58f1ab-f585-402b-bf22-27400acda395",
   "metadata": {},
   "source": [
    "### 3. Lift\n",
    "\n",
    "Lift is the **_observed to expected ratio_** (abbreviation o/e). Lift measures **how likely an item is purchased when another item is purchased, while controlling for how popular both items are**. It can be calculated by dividing the probability of both of the items occurring together by the product of the probabilities of the both individuals items occurring as if there was no association between them.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:660/1*qDMrzlJob5o9K4rnAb1YQg.png)\n",
    "\n",
    "A lift of $1$ will then mean that __both of the items are actually independent and without any association__. \n",
    "\n",
    "For any value __higher than $1$__, lift shows that there is __actually an association__. The __higher the value, the higher the association__.\n",
    "\n",
    "Looking at the table again, the lift of ``{cookies -> cake}`` is $2$, which implies that there is actually an association between ``cookies`` and ``cakes``.\n",
    "\n",
    "Now that we have mastered all the core concepts, we can look into an algorithm that is able to generate item sets from transactional data, which is used to calculate these association rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046fece0-8d5c-4a55-954f-a9e11a9fe0c1",
   "metadata": {},
   "source": [
    "## The Apriori Algorithm\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Apriori Algorithm is one of the most popular algorithms used in association rule learning over relational databases. It **identifies the items in a data set and further extends them to larger and larger item sets**.\n",
    "\n",
    "However, the Apriori Algorithm only extends if the item sets are frequent, that is the probability of the itemset is beyond a certain predetermined threshold.\n",
    "\n",
    "More formally,\n",
    "\n",
    "**The Apriori Algorithm proposes that:**\n",
    "\n",
    "The probability of an itemset is not frequent if:\n",
    "\n",
    "- $P(I) <$ Minimum support threshold, where I is any non-empty itemset\n",
    "- Any subset within the itemset has value less than minimum support.\n",
    "\n",
    "The second characteristic is defined as the **Anti-monotone Property**!\n",
    "\n",
    "A **good example** would be if the probability of purchasing a ``burger`` is below the minimum support already, the probability of purchasing a ``burger`` and ``fries`` will definitely be __below the minimum support as well__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756b3334-d571-4358-a317-f19187b54d90",
   "metadata": {},
   "source": [
    "## Steps in the Apriori Algorithm\n",
    "\n",
    "The diagram below illustrates how the **Apriori Algorithm** starts building from the __smallest itemset__ and further extends forward.\n",
    "\n",
    "- The algorithm starts by **generating an itemset through the Join Step**, that is to generate $(K+1)$ itemset from $K$-itemsets. For example, the algorithm generates ``Cookie``, ``Chocolate`` and ``Cake`` in the first iteration.\n",
    "- Immediately after that, the algorithm proceeds with the __Prune Step__, that is to **remove any candidate item set that does not meet the minimum support requirement**. For example, the algorithm will remove ``Cake`` if $Support($ ``Cake`` $)$ is below the predetermined minimum $Support$.\n",
    "\n",
    "It iterates both of the steps until there are no possible further extensions left.\n",
    "\n",
    "Note that this diagram is not the complete version of the transactions table above. It serves as an illustration to help paint the bigger picture of the flow.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:770/1*oGmHkz3QXn-Dxf7WZeuYSg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cba7e8-4166-4806-97e4-1e2dfd18dbdf",
   "metadata": {},
   "source": [
    "## Code Implementation\n",
    "\n",
    "To perform a Market Basket Analysis implementation with the Apriori Algorithm, we will be using the [Groceries dataset](https://www.kaggle.com/datasets/heeraldedhia/groceries-dataset) from Kaggle. The data set was published by Heeral Dedhia on 2020 with a General Public License, version 2.\n",
    "\n",
    "The dataset has $38765$ rows of purchase orders from the grocery stores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0960869a-9b44-41d8-8599-1b2484899a7f",
   "metadata": {},
   "source": [
    "### Import and read data\n",
    "\n",
    "- First of all, let’s import some necessary modules and read the datasets that we have downloaded from Kaggle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace386fc-7b13-4d64-81c2-71d7aca5a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import re\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f42c4-cfa0-4c52-af30-fdf0bd1d7959",
   "metadata": {},
   "outputs": [],
   "source": [
    "basket = pd.read_csv(\"Groceries_dataset.csv\")\n",
    "basket.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c585f-ff60-4d83-9caa-340ccc372b75",
   "metadata": {},
   "source": [
    "### Some EDA\n",
    "Let’s first have a look at the top 10 most selling products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d619d3-2d3b-4f45-bd76-a001582d7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = basket['itemDescription'].value_counts().sort_values(ascending=False)[:10]\n",
    "fig = px.bar(x= x.index, y= x.values)\n",
    "fig.update_layout(title_text= \"Top 10 frequently sold products\", xaxis_title= \"Products\", yaxis_title=\"Count\", width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90750c4c-cc4e-499f-8dbd-ad3518b46bf6",
   "metadata": {},
   "source": [
    "Now let’s explore the higher sales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883acc2f-9ec1-4dd9-9aea-99aa9814eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "basket[\"Year\"] = basket['Date'].str.split(\"-\").str[-1]\n",
    "basket[\"Month-Year\"] = basket['Date'].str.split(\"-\").str[1] + \"-\" + basket['Date'].str.split(\"-\").str[-1]\n",
    "fig1 = px.bar(basket[\"Month-Year\"].value_counts(ascending=False), \n",
    "              orientation= \"v\", \n",
    "              color = basket[\"Month-Year\"].value_counts(ascending=False),\n",
    "               labels={'value':'Count', 'index':'Date','color':'Meter'})\n",
    "\n",
    "fig1.update_layout(title_text=\"Exploring higher sales by the date\", width=800, height=600)\n",
    "\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae5e36a-1324-4d67-b23d-3e41125fa59a",
   "metadata": {},
   "source": [
    "### Grouping into transactions\n",
    "\n",
    "- The data set records individual item purchases in a row. We will have to group these purchases into baskets of items.\n",
    "- After that, we will use ``TransactionEncoder`` to encode the transactions into a format that is suitable for the Apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6493b1-3b4c-4bb3-b7b0-851b696ecc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "basket.itemDescription = basket.itemDescription.transform(lambda x: [x])\n",
    "basket = basket.groupby(['Member_number','Date']).sum()['itemDescription'].reset_index(drop=True)\n",
    "\n",
    "encoder = TransactionEncoder()\n",
    "encoded_data = encoder.fit_transform(basket)\n",
    "transactions = pd.DataFrame(encoded_data, columns=encoder.columns_)\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d9551-68fa-4ef7-81b6-d8e66522a649",
   "metadata": {},
   "source": [
    "**_Note:_** The data frame records each row as a transaction, and the items that were purchased in the transaction will be recorded as ``True``.\n",
    "\n",
    "### Apriori and Association Rules\n",
    "\n",
    "- The Apriori Algorithm will be used to generate frequent item sets. We will be specifying the minimum support to be $6$ out of total transactions. The association rules are generated and we filter for **Lift** value $> 1.5$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962125aa-55f7-41a9-b14d-34dc84b0e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(transactions, min_support= 6/len(basket), use_colnames=True, max_len = 2)\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\",  min_threshold = 1.5)\n",
    "print(\"Rules identified: \", len(rules))\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14ea572-58af-44a3-99e7-a70f77cb6bdc",
   "metadata": {},
   "source": [
    "* ``support``: Typically, support is used to measure the abundance or frequency (often interpreted as significance or importance) of an itemset in a database. We refer to an itemset as a \"frequent itemset\" if you support is larger than a specified minimum-support threshold. Note that in general, due to the downward closure property, all subsets of a frequent itemset are also frequent. In other words, it is simply the **probability that a customer will buy an item**. The mathematical formula to represent support of item X is:\n",
    "$$\n",
    "\\text{support}(A\\rightarrow C) = \\text{support}(A \\cup C), \\;\\;\\; \\text{range: } [0, 1]\n",
    "$$\n",
    "\n",
    "* ``confidence``: The confidence of a rule $A \\rightarrow C$ is the probability of seeing the consequent in a transaction given that it also contains the antecedent. Note that the metric is not symmetric or directed; for instance, the confidence for $A \\rightarrow C$ is different than the confidence for $C \\rightarrow A$. The confidence is $1$ (maximal) for a rule $A \\rightarrow C$ if the consequent and antecedent always occur together. In other words, it tells us the **impact of one product on another** that is the probability that if a person buys product $X$ then he/she will buy product $Y$ also. Its representation in mathematical terms is:\n",
    "$$\n",
    "\\text{confidence}(A\\rightarrow C) = \\frac{\\text{support}(A\\rightarrow C)}{\\text{support}(A)}, \\;\\;\\; \\text{range: } [0, 1]\n",
    "$$\n",
    "\n",
    "* ``lift``: The lift metric is commonly used to measure how much more often the antecedent and consequent of a rule $A \\rightarrow C$ occur together than we would expect if they were statistically independent. If $A$ and $C$ are independent, the **Lift** score will be exactly $1$. In other words, Lift will calculate the **confidence taking into account the popularity of both items**. Representation of lift in mathematical terms is:\n",
    "$$\n",
    "\\text{lift}(A\\rightarrow C) = \\frac{\\text{confidence}(A\\rightarrow C)}{\\text{support}(C)}, \\;\\;\\; \\text{range: } [0, \\infty]\n",
    "$$\n",
    "\n",
    "* ``leverage``: Leverage computes the difference between the observed frequency of $A$ and $C$ appearing together and the frequency that would be expected if $A$ and $C$ were independent. A leverage value of $0$ indicates independence. The mathematical formula is as follows:\n",
    "$$\n",
    "\\text{levarage}(A\\rightarrow C) = \\text{support}(A\\rightarrow C) - \\text{support}(A) \\times \\text{support}(C), \\;\\;\\; \\text{range: } [-1, 1]\n",
    "$$\n",
    "\n",
    "* ``conviction``: A __high conviction value means that the consequent is highly depending on the antecedent__. For instance, in the case of a perfect confidence score, the denominator becomes $0$ (due to $1 - 1$) for which the conviction score is defined as ``inf``. Similar to lift, if items are independent, the conviction is $1$. The mathematical formula is as follows:\n",
    "$$\n",
    "\\text{conviction}(A\\rightarrow C) = \\frac{1 - \\text{support}(C)}{1 - \\text{confidence}(A\\rightarrow C)}, \\;\\;\\; \\text{range: } [0, \\infty]\n",
    "$$\n",
    "\n",
    "* ``zhangs_metric``: Measures both association and dissociation. Value ranges between $-1$ and $1$. A positive value $(>0)$ indicates **Association** and negative value indicated **Dissociation**. The mathematical formula is as follows:\n",
    "$$\n",
    "\\text{zhangs metric}(A\\rightarrow C) = \\frac{\\text{confidence}(A\\rightarrow C) - \\text{confidence}(A'\\rightarrow C)}{Max[ \\text{confidence}(A\\rightarrow C) , \\text{confidence}(A'\\rightarrow C)]}, \\;\\;\\; \\text{range: } [-1, 1]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59505389-9053-4b2c-9f21-47480b733686",
   "metadata": {},
   "source": [
    "### Visualizations\n",
    "\n",
    "To visualize our association rules, we can plot them in a 3D scatter plot. Rules that are closer to top right are the rules that can be the most meaningful to be further dived in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ec4af-e596-40e1-b74e-a01d8b2965ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(projection = '3d')\n",
    "\n",
    "x = rules['support']\n",
    "y = rules['confidence']\n",
    "z = rules['lift']\n",
    "\n",
    "ax.set_xlabel(\"Support\")\n",
    "ax.set_ylabel(\"Confidence\")\n",
    "ax.set_zlabel(\"Lift\")\n",
    "\n",
    "ax.scatter(x, y, z)\n",
    "ax.set_title(\"3D Distribution of Association Rules\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef7de5f-b7d2-4e49-b999-aa0679507984",
   "metadata": {},
   "source": [
    "Another type of visualizations to look at the relationship between the products is via **Network Graph**. \n",
    "\n",
    "Let’s define a function to draw a network graph which can specify how many rules we want to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd983410-c974-4f0b-aee3-0bcb31c22219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_network(rules, rules_to_show):\n",
    "  # Directional Graph from NetworkX\n",
    "  network = nx.DiGraph()\n",
    "  \n",
    "  # Loop through number of rules to show\n",
    "  for i in range(rules_to_show):\n",
    "    \n",
    "    # Add a Rule Node\n",
    "    network.add_nodes_from([\"R\"+str(i)])\n",
    "    for antecedents in rules.iloc[i]['antecedents']: \n",
    "        # Add antecedent node and link to rule\n",
    "        network.add_nodes_from([antecedents])\n",
    "        network.add_edge(antecedents, \"R\"+str(i),  weight = 2)\n",
    "      \n",
    "    for consequents in rules.iloc[i]['consequents']:\n",
    "        # Add consequent node and link to rule\n",
    "        network.add_nodes_from([consequents])\n",
    "        network.add_edge(\"R\"+str(i), consequents,  weight = 2)\n",
    "\n",
    "  color_map=[]  \n",
    "  \n",
    "  # For every node, if it's a rule, colour as Black, otherwise Orange\n",
    "  for node in network:\n",
    "       if re.compile(r\"^[R]\\d+$\").fullmatch(node) != None:\n",
    "            color_map.append('black')\n",
    "       else:\n",
    "            color_map.append('orange')\n",
    "  \n",
    "  # Position nodes using spring layout\n",
    "  pos = nx.spring_layout(network, k=16, scale=1)\n",
    "  # Draw the network graph\n",
    "  nx.draw(network, pos, node_color = color_map, font_size=8)            \n",
    "  \n",
    "  # Shift the text position upwards\n",
    "  for p in pos:  \n",
    "      pos[p][1] += 0.12\n",
    "\n",
    "  nx.draw_networkx_labels(network, pos)\n",
    "  plt.title(\"Network Graph for Association Rules\")\n",
    "  plt.show()\n",
    "\n",
    "draw_network(rules, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172887ba-ff01-43f8-a5e2-2685031ea1ca",
   "metadata": {},
   "source": [
    "### Business Application\n",
    "\n",
    "Let’s say the grocery has bought up too much ``Whole Milk`` and is now worrying that the stocks will expire if they cannot be sold out in time. To make matters worse, the profit margin of ``Whole Milk`` is so low that they cannot afford to have a promotional discount without killing too much of their profits.\n",
    "\n",
    "One approach that can be proposed is to find out which products drive the sales of ``Whole Milk`` and offer discounts on those products instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2e106-0a49-4ab4-95ab-2f97fe284320",
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_rules = rules[rules['consequents'].astype(str).str.contains('whole milk')]\n",
    "milk_rules = milk_rules.sort_values(by=['lift'], ascending=False).reset_index(drop=True)\n",
    "milk_rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d0c127-71c5-4a71-b849-a879115ad064",
   "metadata": {},
   "source": [
    "For instance, we can apply a promotional discount on ``Brandy``, ``Softener``, ``Canned Fruit``, ``Syrup`` and ``Artificial Sweetener``. Some of the associations may seem counter-intuitive, but the rules state that these products do drive the sales of ``Whole Milk``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f53df-9556-4c6f-abb9-b1bade5bae3f",
   "metadata": {},
   "source": [
    "## Another example - Movie Recommender using Apriori Algorithm\n",
    "\n",
    "_Retrieved from Kaggle: https://www.kaggle.com/code/ankits29/movie-recommendation-with-ml-apriori-explained_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9871700-25c0-4b81-afcb-326f8eb978fa",
   "metadata": {
    "papermill": {
     "duration": 0.027627,
     "end_time": "2021-02-14T05:41:16.583667",
     "exception": false,
     "start_time": "2021-02-14T05:41:16.556040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The dataset at hand contains records of movies watched by users and their ratings. Your job is to extract relations of the movies watched by a user and recommend movies to a user based on the previously watched movies. This is same as youtube recommending videos to you saying people who watched this video also watched this, or maybe like Netflix or Amazon prime recommending you other movies or series based on your watch history and of others who have watched the same movies as you.\n",
    "\n",
    "Let's begin creating a recommendation system for movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e05da-3e28-4843-847f-80a07950257d",
   "metadata": {
    "papermill": {
     "duration": 0.876835,
     "end_time": "2021-02-14T05:41:17.485821",
     "exception": false,
     "start_time": "2021-02-14T05:41:16.608986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cabfb9a-c3d3-41af-93a4-4947891f3bad",
   "metadata": {
    "papermill": {
     "duration": 0.025729,
     "end_time": "2021-02-14T05:41:17.537316",
     "exception": false,
     "start_time": "2021-02-14T05:41:17.511587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e126c4f-9213-4395-b27f-019f66986fcb",
   "metadata": {
    "papermill": {
     "duration": 1.425157,
     "end_time": "2021-02-14T05:41:18.988100",
     "exception": false,
     "start_time": "2021-02-14T05:41:17.562943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv('ratings_small.csv')\n",
    "movies_df = pd.read_csv('movies_metadata.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1123a7-0f6c-47d3-821d-b9d6eb677d1e",
   "metadata": {
    "papermill": {
     "duration": 0.061654,
     "end_time": "2021-02-14T05:41:19.078378",
     "exception": false,
     "start_time": "2021-02-14T05:41:19.016724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d883e99-1e56-461d-8914-dedc7685f824",
   "metadata": {
    "papermill": {
     "duration": 0.062806,
     "end_time": "2021-02-14T05:41:19.247229",
     "exception": false,
     "start_time": "2021-02-14T05:41:19.184423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a011db4-dff8-47f3-93f3-815b39e95e07",
   "metadata": {
    "papermill": {
     "duration": 0.028192,
     "end_time": "2021-02-14T05:41:19.462880",
     "exception": false,
     "start_time": "2021-02-14T05:41:19.434688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The ratings dataframe contains information of userId, the movieId of the movie watched by that user, the rating given by the user and timestamp.\n",
    "\n",
    "The movies dataframe contains the information of the movies like movieId, title, genre and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5ec3a-2def-4239-8f0b-14ba1cda5e17",
   "metadata": {
    "papermill": {
     "duration": 0.320166,
     "end_time": "2021-02-14T05:41:19.811383",
     "exception": false,
     "start_time": "2021-02-14T05:41:19.491217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "ax = sns.countplot(data=ratings_df, x='rating')\n",
    "labels = (ratings_df['rating'].value_counts().sort_index())\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.xlabel('Ratings')\n",
    "\n",
    "for i,v in enumerate(labels):\n",
    "    ax.text(i, v+100, str(v), horizontalalignment='center', size=14, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf20dd9-97d0-4d24-90a3-2d3e39bede85",
   "metadata": {
    "papermill": {
     "duration": 0.029354,
     "end_time": "2021-02-14T05:41:19.879425",
     "exception": false,
     "start_time": "2021-02-14T05:41:19.850071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The ratings distribution shows that there are relatively fewer movies that are lower rated. This can be because most of the users who didn't like the movie, didn't care enough to rate the movie. You should note this, it can be useful later. As you wouldn't want to recommend movies with relatively low number of ratings as users probably didn't like them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b303145b-4e0f-4d02-b55c-57a1dea31039",
   "metadata": {
    "papermill": {
     "duration": 0.029203,
     "end_time": "2021-02-14T05:41:19.938114",
     "exception": false,
     "start_time": "2021-02-14T05:41:19.908911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Clean the Data\n",
    "\n",
    "You can see that in the movies dataframe, there are few records with Nan title. This doesn't serve your purpose as you cannot recommend movies without title. You can remove these records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8f5db-dadc-4561-a47d-e57991d90609",
   "metadata": {
    "papermill": {
     "duration": 0.042555,
     "end_time": "2021-02-14T05:41:20.010256",
     "exception": false,
     "start_time": "2021-02-14T05:41:19.967701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_mask = movies_df['title'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72907942-06f8-49ca-bb40-dd32e123f459",
   "metadata": {
    "papermill": {
     "duration": 0.071129,
     "end_time": "2021-02-14T05:41:20.111124",
     "exception": false,
     "start_time": "2021-02-14T05:41:20.039995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies_df = movies_df.loc[title_mask == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b1c416-53eb-4689-9452-824710128a75",
   "metadata": {
    "papermill": {
     "duration": 0.029412,
     "end_time": "2021-02-14T05:41:20.170270",
     "exception": false,
     "start_time": "2021-02-14T05:41:20.140858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You would also like to merge the two dataframes so that you have a dataframe having userId and the title of the movie watched by the user. If you know SQL, you might be familiar with the concept of join. You can merge the two dataframe on a common column -> movieId. As a result, you will have the records of ratings dataframe concatenated with the corresponding details of the movie from the movies dataframe and the way it gets to know the corresponding record is by using the common column movieId.\n",
    "\n",
    "Before merging you need to convert the string datatype of id column of movies dataframe to int as that in the ratings dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaafb573-effb-4bdf-83a3-2ce1099166b6",
   "metadata": {
    "papermill": {
     "duration": 0.082953,
     "end_time": "2021-02-14T05:41:20.283192",
     "exception": false,
     "start_time": "2021-02-14T05:41:20.200239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies_df = movies_df.astype({'id': 'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f328b-a2f4-47cd-aabf-e00bcc170a74",
   "metadata": {
    "papermill": {
     "duration": 0.068897,
     "end_time": "2021-02-14T05:41:20.382231",
     "exception": false,
     "start_time": "2021-02-14T05:41:20.313334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.merge(ratings_df, movies_df[['id', 'title']], left_on='movieId', right_on='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369cba49-a8c0-4842-aaa7-626ae88b8882",
   "metadata": {
    "papermill": {
     "duration": 0.030561,
     "end_time": "2021-02-14T05:41:20.443596",
     "exception": false,
     "start_time": "2021-02-14T05:41:20.413035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Id column is repeated and the timestamp is not important for this problem. So, you can drop the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b761f3-6abe-475d-8041-a58df2e13cae",
   "metadata": {
    "papermill": {
     "duration": 0.042137,
     "end_time": "2021-02-14T05:41:20.516438",
     "exception": false,
     "start_time": "2021-02-14T05:41:20.474301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(['timestamp', 'id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d2056-c842-4e78-8710-55edef67fecd",
   "metadata": {
    "papermill": {
     "duration": 0.031395,
     "end_time": "2021-02-14T05:41:20.579559",
     "exception": false,
     "start_time": "2021-02-14T05:41:20.548164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The apriori model needs data in a format such that the userId forms the index, the columns are the movie titles and the values can be 1 or 0 depending on whether that user has watched the movie of the corresponding column. The resulting data is like a user's watchlist, for each userId, having 1 in columns of the movies that the user has watched and 0 otherwise.\n",
    "\n",
    "You can achieve this by using pivot on the dataframe. To do so you need to first make sure there are no duplicate records for the combination of userId and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b48eea4-172f-40a8-8e4c-740473aea670",
   "metadata": {
    "papermill": {
     "duration": 0.048379,
     "end_time": "2021-02-14T05:41:20.659442",
     "exception": false,
     "start_time": "2021-02-14T05:41:20.611063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(['userId','title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46817bf1-eff9-4379-8189-d673a2cdc28e",
   "metadata": {
    "papermill": {
     "duration": 0.095062,
     "end_time": "2021-02-14T05:41:20.785618",
     "exception": false,
     "start_time": "2021-02-14T05:41:20.690556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pivot = df.pivot(index='userId', columns='title', values='rating').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77808686-a55b-4831-90d8-c3ba4d10f99a",
   "metadata": {
    "papermill": {
     "duration": 0.031367,
     "end_time": "2021-02-14T05:41:20.849319",
     "exception": false,
     "start_time": "2021-02-14T05:41:20.817952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You need to convert the ratings to 0 or 1 and also convert all float values to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d9602-022d-4bb0-9617-86bcacedef10",
   "metadata": {
    "papermill": {
     "duration": 0.046784,
     "end_time": "2021-02-14T05:41:20.927527",
     "exception": false,
     "start_time": "2021-02-14T05:41:20.880743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pivot = df_pivot.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd9868-095b-4a0a-a07d-29c2d630a8e8",
   "metadata": {
    "papermill": {
     "duration": 1.87028,
     "end_time": "2021-02-14T05:41:22.829162",
     "exception": false,
     "start_time": "2021-02-14T05:41:20.958882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_ratings(x):\n",
    "    if x<=0:\n",
    "        return 0\n",
    "    if x>=1:\n",
    "        return 1\n",
    "\n",
    "df_pivot = df_pivot.applymap(encode_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f66846-8e16-4284-b21d-a745df0e4a48",
   "metadata": {
    "papermill": {
     "duration": 0.055389,
     "end_time": "2021-02-14T05:41:22.916994",
     "exception": false,
     "start_time": "2021-02-14T05:41:22.861605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c3311-302f-482c-baea-216ca5dba487",
   "metadata": {
    "papermill": {
     "duration": 0.032346,
     "end_time": "2021-02-14T05:41:22.982022",
     "exception": false,
     "start_time": "2021-02-14T05:41:22.949676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Your data looks to be ready now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9c2d4a8-f088-441a-aa57-82b368c2fa4e",
   "metadata": {
    "papermill": {
     "duration": 0.032121,
     "end_time": "2021-02-14T05:41:23.046877",
     "exception": false,
     "start_time": "2021-02-14T05:41:23.014756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Train the Model\n",
    "\n",
    "The apriori model calculates the probability to determine how likely a user will watch movie M2 if he has already watched a movie M1. It does so by computing support, confidence and lift for different combinations of movies.\n",
    "\n",
    "* __Support__ of a movie $M1$ is like the probability of users watching movie M1. Support computes, out of the total users, how much percentage of users have movie M1 in their watchlist.\n",
    "* __Confidence__ of a movie is out of the total users having watched movie M1, how many have also watched movie M2. It is denoted as Confidence(M1 -> M2).\n",
    "* __Lift__ is the ratio of __confidence__ and __support__.\n",
    "\n",
    "From the definition of support, you know that Support(M2) is the likelihood of users watching movie M2 if you recommend it to all the users.\n",
    "\n",
    "While Confidence(M1 -> M2), is the likelihood of users watching movie M2 if you recommend it to only the users who have already watched movie M1. In confidence you recommend the movie to a subset of population.\n",
    "\n",
    "Lift then by definition is the measure of increase in likelihood of users watching the movie M2 when we recommend it to the subset than when we recommend it to entire population. So a high lift suggests there is some relation between the two movies and most of the users who have watched movie M1 are also likely to watch movie M2.\n",
    "\n",
    "This is for just one pair, the model has to compute this for every possible combination of movies to recommend the movies that the user will most likely watch.\n",
    "\n",
    "Looks like your model has to do a lot of computation.\n",
    "\n",
    "You can ease its job by using a threshold for a minimum support. As you have seen that movies with low rating have less number of reviews. So, you don't want to bother the model to recommend such movies which users don't like. Setting a minimum support ensures that atleast some percentage of users have watched that movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb129a-4912-4eae-b05b-bfd1ec5eed50",
   "metadata": {
    "papermill": {
     "duration": 23.650518,
     "end_time": "2021-02-14T05:41:46.729823",
     "exception": false,
     "start_time": "2021-02-14T05:41:23.079305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "frequent_itemset = apriori(df_pivot, min_support=0.07, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd44622-4011-4057-8839-0a7f89bd91a9",
   "metadata": {
    "papermill": {
     "duration": 0.048175,
     "end_time": "2021-02-14T05:41:46.811689",
     "exception": false,
     "start_time": "2021-02-14T05:41:46.763514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "frequent_itemset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be52a5bd-6bbd-4cd8-85b0-ed54603d9939",
   "metadata": {
    "papermill": {
     "duration": 0.033227,
     "end_time": "2021-02-14T05:41:46.878454",
     "exception": false,
     "start_time": "2021-02-14T05:41:46.845227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The apriori algorithm has given you the support, using association_rules you can compute the other paramters like confidence and lift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55914b-20eb-4043-834b-36d43e36b3e3",
   "metadata": {
    "papermill": {
     "duration": 29.998485,
     "end_time": "2021-02-14T05:42:16.910414",
     "exception": false,
     "start_time": "2021-02-14T05:41:46.911929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "rules = association_rules(frequent_itemset, metric=\"lift\", min_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e79dbe-ae03-4314-9bf9-fcfce82690ee",
   "metadata": {
    "papermill": {
     "duration": 0.055331,
     "end_time": "2021-02-14T05:42:16.999022",
     "exception": false,
     "start_time": "2021-02-14T05:42:16.943691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f82525-6758-4960-8f1e-e52340b32e18",
   "metadata": {
    "papermill": {
     "duration": 0.033285,
     "end_time": "2021-02-14T05:42:17.066286",
     "exception": false,
     "start_time": "2021-02-14T05:42:17.033001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Interpret the Results\n",
    "\n",
    "Let's sort the result by descending order of lift. So that the most likely movie that the user will watch is recommended first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b1112-3acc-4216-88a2-e0e6c2e0f8fa",
   "metadata": {
    "papermill": {
     "duration": 1.885433,
     "end_time": "2021-02-14T05:42:18.985166",
     "exception": false,
     "start_time": "2021-02-14T05:42:17.099733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_res = rules.sort_values(by=['lift'], ascending=False)\n",
    "df_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c779e1-88db-494b-9636-44d01171436d",
   "metadata": {
    "papermill": {
     "duration": 0.033808,
     "end_time": "2021-02-14T05:42:19.053454",
     "exception": false,
     "start_time": "2021-02-14T05:42:19.019646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's see what your model recommends to someone who has watched the **Men in Black II**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c355cdaf-935a-4e53-8032-e15a58173c6d",
   "metadata": {
    "papermill": {
     "duration": 1.502071,
     "end_time": "2021-02-14T05:42:20.589545",
     "exception": false,
     "start_time": "2021-02-14T05:42:19.087474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_MIB = df_res[df_res['antecedents'].apply(lambda x: len(x) ==1 and next(iter(x)) == 'Men in Black II')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cffabcd-cd58-464e-ab2b-cdab165fbace",
   "metadata": {
    "papermill": {
     "duration": 0.043346,
     "end_time": "2021-02-14T05:42:20.667434",
     "exception": false,
     "start_time": "2021-02-14T05:42:20.624088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_MIB = df_MIB[df_MIB['lift'] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a13e8a-8d05-47f4-bbb9-1d8158541569",
   "metadata": {
    "papermill": {
     "duration": 0.057867,
     "end_time": "2021-02-14T05:42:20.760747",
     "exception": false,
     "start_time": "2021-02-14T05:42:20.702880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_MIB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f0b2d2-7c9e-4cea-a3ee-e6be59a6bacb",
   "metadata": {
    "papermill": {
     "duration": 0.034973,
     "end_time": "2021-02-14T05:42:20.832012",
     "exception": false,
     "start_time": "2021-02-14T05:42:20.797039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You have a bunch of recommendation there. Let's have a list of unique movies in the order of descending lift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d520e0ab-b548-405f-b658-93f6b037e5d1",
   "metadata": {
    "papermill": {
     "duration": 0.045615,
     "end_time": "2021-02-14T05:42:20.913856",
     "exception": false,
     "start_time": "2021-02-14T05:42:20.868241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies = df_MIB['consequents'].values\n",
    "\n",
    "movie_list = []\n",
    "for movie in movies:\n",
    "    for title in movie:\n",
    "        if title not in movie_list:\n",
    "            movie_list.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de917983-1feb-414a-a00e-39c3e7f57624",
   "metadata": {
    "papermill": {
     "duration": 0.044871,
     "end_time": "2021-02-14T05:42:20.994179",
     "exception": false,
     "start_time": "2021-02-14T05:42:20.949308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "movie_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41be50-1dba-4051-bd0e-837b47112faa",
   "metadata": {
    "papermill": {
     "duration": 0.036106,
     "end_time": "2021-02-14T05:42:21.066245",
     "exception": false,
     "start_time": "2021-02-14T05:42:21.030139",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Great! You have the top 10 movies that the user is most likely to watch. The result looks convincing to me.\n",
    "\n",
    "We have a our own recommendation system now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
