{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f635ec-6c4d-4730-97a1-645780d9dc70",
   "metadata": {},
   "source": [
    "# Q-Learning vs. SARSA (Cliff Walking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de86322-884d-4158-9bc9-a179b590a991",
   "metadata": {},
   "source": [
    "### Q-Learning Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e931b-394b-4370-8c7f-f6a81c1ec092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "# Create Cliff Walking environment\n",
    "env = gym.make(\"CliffWalking-v0\")\n",
    "\n",
    "# Initialize Q-table\n",
    "state_size = env.observation_space.n  # Total number of states\n",
    "action_size = env.action_space.n      # Total number of actions\n",
    "q_table = np.zeros((state_size, action_size))\n",
    "print(\"State space:\", state_size)\n",
    "print(\"Action space:\", action_size)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1  # Alpha, learning rate\n",
    "discount_rate = 0.99  # Gamma, discount factor\n",
    "epsilon = 1.0       # Exploration rate\n",
    "decay_rate = 0.0001    # Decay rate for epsilon\n",
    "\n",
    "# Training variables\n",
    "num_episodes = 2000  # Total number of episodes\n",
    "max_steps = 40      # Max steps per episode\n",
    "epsilon_decayed = 1.0\n",
    "\n",
    "# To store total rewards for each episode\n",
    "rewards = []\n",
    "\n",
    "# Policy function for epsilon-greedy action selection\n",
    "def policy(state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()  # Explore\n",
    "    else:\n",
    "        return np.argmax(q_table[state])  # Exploit\n",
    "\n",
    "# Training the agent\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()  # Reset the environment to the initial state\n",
    "    done = False               # Variable to check if the episode is finished\n",
    "    total_reward = 0\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        action = policy(state, epsilon_decayed)\n",
    "        next_state, reward, done, truncated, info = env.step(action)  # Take action and observe the result\n",
    "\n",
    "        # Update Q-table\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (reward + discount_rate * np.max(q_table[next_state]) - q_table[state, action])\n",
    "        \n",
    "        state = next_state  # Move to the next state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Render the environment\n",
    "        clear_output(wait=True)\n",
    "        # env.render()\n",
    "        # time.sleep(0.01)\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "    epsilon_decayed = np.exp(-decay_rate * episode)\n",
    "    print(f\"Episode {episode + 1}: Total Reward: {total_reward}, Epsilon: {epsilon_decayed}\")\n",
    "\n",
    "# Save the Q-table\n",
    "pkl.dump(q_table, open(\"q-learning_q_table.pkl\", \"wb\"))\n",
    "print(\"Training Complete! Q-table saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10819ce9-cceb-415d-b94c-e33938135bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting rewards over time\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Rewards over Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b726a20-8e6f-45ea-8b6c-f282955563c8",
   "metadata": {},
   "source": [
    "### Q-Learning Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d868e9e4-dc96-43ca-9c57-0c5b98eba6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import pickle as pkl\n",
    "import gym\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "file = open('q-learning_q_table.pkl', 'rb')\n",
    "q_table = pkl.load(file)\n",
    "\n",
    "# Trained variables\n",
    "max_steps = 40\n",
    "\n",
    "# Watch the trained agent\n",
    "env = gym.make(\"CliffWalking-v0\", render_mode='human')\n",
    "state, info = env.reset()\n",
    "done = False\n",
    "rewards = 0\n",
    "\n",
    "for s in range(max_steps):\n",
    "    clear_output(wait=True)\n",
    "    env.render()  # Render the environment in the human mode\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    rewards += reward\n",
    "    \n",
    "    print(f\"Step {s+1}, Total Reward: {rewards}\")\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    if done or truncated:\n",
    "        pygame.quit()\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605f4b18-038d-430d-b6fc-627a335eef15",
   "metadata": {},
   "source": [
    "### SARSA Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8647b-5fa7-4772-995b-1ce3dbafdf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import pickle as pkl\n",
    "import gym\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "file = open('sarsa_q_table.pkl', 'rb')\n",
    "q_table = pkl.load(file)\n",
    "\n",
    "# Trained variables\n",
    "max_steps = 40\n",
    "\n",
    "# Watch the trained agent\n",
    "env = gym.make(\"CliffWalking-v0\", render_mode='human')\n",
    "state, info = env.reset()\n",
    "done = False\n",
    "rewards = 0\n",
    "\n",
    "for s in range(max_steps):\n",
    "    clear_output(wait=True)\n",
    "    env.render()  # Render the environment in the human mode\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    rewards += reward\n",
    "    print(f\"Step {s+1}, Total Reward: {rewards}\")\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    if done or truncated:\n",
    "        pygame.quit()\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
