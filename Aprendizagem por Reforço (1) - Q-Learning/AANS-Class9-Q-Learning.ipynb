{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e1bcf74-3a1b-4fe1-b1e5-0a74d50f1d5a",
   "metadata": {},
   "source": [
    "# An Introduction to Reinforcement Learning\n",
    "\n",
    "Think about how you might teach a dog a new trick, like telling it to sit:\n",
    "\n",
    "- If it performs the trick correctly (it sits), you'll reward it with a treat (positive feedback) ✔️\n",
    "- If it doesn't sit correctly, it doesn't get a treat (negative feedback) ❌\n",
    "\n",
    "By continuing to do things that lead to positive outcomes, the dog will learn to sit when it hears the command in order to get its treat. Reinforcement learning is a subdomain of machine learning which involves training an 'agent' (the dog) to learn the correct sequences of actions to take (sitting) on its environment (in response to the command 'sit') in order to maximize its reward (getting a treat).\n",
    "\n",
    "This can be illustrated more formally as:\n",
    "\n",
    "![sutton barto rl](https://www.gocoder.one/static/RL-diagram-b3654cd3d5cc0e07a61a214977038f01.png)\n",
    "\n",
    "## Key Concepts in Reinforcement Learning\n",
    "\n",
    "1. **Agent**: The learner or decision maker.\n",
    "2. **Environment**: The external system with which the agent interacts.\n",
    "3. **State (s)**: A representation of the current situation of the agent.\n",
    "4. **Action (a)**: Choices made by the agent that affect the environment.\n",
    "5. **Reward (r)**: Feedback from the environment based on the action taken.\n",
    "6. **Policy (π)**: A strategy used by the agent to determine the next action based on the current state.\n",
    "7. **Value Function (V)**: A function that estimates the expected cumulative reward from a given state.\n",
    "8. **Q-Value (Q)**: A function that estimates the expected cumulative reward from a given state-action pair.\n",
    "\n",
    "## Detailed Explanation of Key Concepts\n",
    "\n",
    "- **Agent**: In RL, the agent is the entity that interacts with the environment to learn optimal behaviors. The agent's goal is to maximize the cumulative reward it receives over time.\n",
    "- **Environment**: The environment is the external system that the agent interacts with. It provides the agent with states and rewards based on the actions taken.\n",
    "- **State (s)**: A state is a specific situation or configuration of the environment. The state provides the agent with all the information needed to make a decision.\n",
    "- **Action (a)**: An action is a specific move or decision made by the agent that affects the state of the environment.\n",
    "- **Reward (r)**: A reward is a scalar feedback signal received by the agent after taking an action. It indicates how good or bad the action was in the context of the environment.\n",
    "- **Policy (π)**: A policy is a mapping from states to actions. It defines the agent's behavior at any given time. The policy can be deterministic (specific action for each state) or stochastic (probability distribution over actions for each state).\n",
    "- **Value Function (V)**: The value function estimates the expected cumulative reward from a given state, following a specific policy. It helps the agent evaluate how good it is to be in a certain state.\n",
    "- **Q-Value (Q)**: The Q-value (or action-value) function estimates the expected cumulative reward from a given state-action pair. It helps the agent evaluate how good it is to take a certain action in a certain state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff22601f-4ae4-46c2-b60b-246ca6b2122f",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Q-learning is a reinforcement learning algorithm that seeks to find the best possible next action given its current state, in order to maximise the reward it receives (the 'Q' in Q-learning stands for quality - i.e. how valuable an action is).\n",
    "\n",
    "Let's take the following starting state:\n",
    "\n",
    "![](https://www.gocoder.one/static/start-state-6a115a72f07cea072c28503d3abf9819.png)\n",
    "\n",
    "Which action (up, down, left, right, pick-up or drop-off) should it take in order to maximise its reward? (_Note: blue = pick-up location and green= drop-off destination_)\n",
    "\n",
    "First, let's take a look at how our agent is 'rewarded' for its actions. **Remember in reinforcement learning, we want our agent to take actions that will maximise the possible rewards it receives from its environment.**\n",
    "\n",
    "### 'Taxi' reward system\n",
    "\n",
    "According to the [Taxi documentation](https://gym.openai.com/envs/Taxi-v3/):\n",
    "\n",
    "> _\"…you receive +20 points for a successful drop-off, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\"_\n",
    "\n",
    "Looking back at our original state, the possible actions it can take and the corresponding rewards it will receive are shown below:\n",
    "\n",
    "![](https://www.gocoder.one/static/state-rewards-62ab43a53e07062b531b3199a8bab5b3.png)\n",
    "\n",
    "In the image above, the agent loses 1 point per timestep it takes. It will also lose 10 points if it uses the pick-up or drop-off action here.\n",
    "\n",
    "We want our agent to go North towards the pick-up location denoted by a blue R - **but how will it know which action to take if they are all equally punishing?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e447f36f-76a2-4291-b29f-03db4c08a4bc",
   "metadata": {},
   "source": [
    "### Exploration vs. Exploitation\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/exp_2.jpg\" width=\"600\">\n",
    "\n",
    "#### Exploration\n",
    "\n",
    "Our agent currently has no way of knowing which action will lead it closest to the blue R. This is where trial-and-error comes in - we'll have our agent take random actions, and observe what rewards it gets (i.e. our agent will **explore**).\n",
    "\n",
    "Over many iterations, our agent will have observed that certain sequences of actions will be more rewarding than others. Along the way, our agent will need to keep track of which actions led to what rewards.\n",
    "\n",
    "#### Exploitation\n",
    "\n",
    "We can let our agent explore to update our Q-table using the Q-learning algorithm. As our agent learns more about the environment, we can let it use this knowledge to take more optimal actions and converge faster - known as **exploitation**.\n",
    "\n",
    "During exploitation, our agent will look at its Q-table and select the action with the highest Q-value (instead of a random action). Over time, our agent will need to explore less, and start exploiting what it knows instead.\n",
    "\n",
    "### Epsilon-Greedy ($\\epsilon$) Exploration vs. Exploitation\n",
    "\n",
    "In reinforcement learning, an agent must balance between exploring new actions to discover their effects (**exploration**) and exploiting known actions that yield high rewards (**exploitation**). The epsilon-greedy strategy is commonly used to address this trade-off.\n",
    "\n",
    "#### Epsilon-Greedy Strategy\n",
    "\n",
    "-   **Exploration**: With probability $\\epsilon$, the agent chooses a random action to explore the environment. This helps the agent discover new strategies and avoid local optima.\n",
    "-   **Exploitation**: With probability 1−$\\epsilon_1$ - $\\epsilon$, the agent chooses the action with the highest Q-value. This leverages the knowledge already gained to maximize the reward.\n",
    "\n",
    "#### Formula\n",
    "\n",
    "$$ \\text{action} =\n",
    "\\begin{cases} \n",
    "\\text{random action} & \\text{with probability } \\epsilon \\\\\n",
    "\\arg\\max\\limits_a Q(s, a) & \\text{with probability } 1 - \\epsilon\n",
    "\\end{cases} $$\n",
    "\n",
    "#### Epsilon Decay\n",
    "\n",
    "Initially, a higher $\\epsilon$ encourages exploration. Over time, $\\epsilon$ decays to encourage exploitation as the agent gains more knowledge about the environment.\n",
    "\n",
    "#### Visual Explanation\n",
    "\n",
    "1.  **Exploration Phase** (high $\\epsilon$): The agent tries different actions to explore the environment.\n",
    "2.  **Exploitation Phase** (low $\\epsilon$): The agent uses the knowledge gained to choose the best-known actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511463fa-6ab0-4e43-b934-889339c6c421",
   "metadata": {},
   "source": [
    "### Introducing… Q-tables\n",
    "\n",
    "A Q-table is simply a look-up table storing values representing the maximum expected future rewards our agent can expect for a certain action in a certain state (_known as Q-values_). It will tell our agent that when it encounters a certain state, some actions are more likely than others to lead to higher rewards. It becomes a 'cheatsheet' telling our agent what the best action to take is.\n",
    "\n",
    "The image below illustrates what our 'Q-table' will look like:\n",
    "\n",
    "-   Each row corresponds to a unique state in the 'Taxi' environment\n",
    "-   Each column corresponds to an action our agent can take\n",
    "-   Each cell corresponds to the Q-value for that state-action pair - a higher Q-value means a higher maximum reward our agent can expect to get if it takes that action in that state.\n",
    "\n",
    "<img src=\"https://www.gocoder.one/static/q-table-9461cc903f50b78d757ea30aeb3eb8bc.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31ee63-c612-480b-9576-d64edeecece0",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm\n",
    "\n",
    "The Q-learning algorithm is given below. We won't go into details, but you can read more about it in [Ch 6 of Sutton & Barto (2018)](http://www.incompleteideas.net/book/RLbook2018trimmed.pdf).\n",
    "\n",
    "![](https://www.gocoder.one/static/q-learning-algorithm-84b84bb5dc16ba8097e31aff7ea42748.png)\n",
    "\n",
    "The Q-learning algorithm will help our agent **update the current $Q$-value $Q(S_t,A_t)$ with its observations after taking an action.** I.e. increase Q if it encountered a positive reward, or decrease Q if it encountered a negative one.\n",
    "\n",
    "Note that in Taxi, our agent doesn't receive a positive reward until it successfully drops off a passenger (_+20 points_). Hence even if our agent is heading in the correct direction, there will be a delay in the positive reward it should receive. The following term in the Q-learning equation addresses this:\n",
    "\n",
    "![](https://www.gocoder.one/static/max-q-e593ddcec76cda87ed189c31d60837b6.png)\n",
    "\n",
    "This term adjusts our current Q-value to include a portion of the rewards it may receive sometime in the future ($S_t+1$). The '$a$' term refers to all the possible actions available for that state. The equation also contains two hyperparameters which we can specify:\n",
    "\n",
    "1.  Learning rate ($α$): how easily the agent should accept new information over previously learnt information\n",
    "2.  Discount factor ($γ$): how much the agent should take into consideration the rewards it could receive in the future versus its immediate reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18605f2f-0c68-4e10-8161-66d6035629c9",
   "metadata": {},
   "source": [
    "### Steps in Q-Learning\n",
    "\n",
    "1. **Initialize the Q-table**: Assign a value of 0 to all entries.\n",
    "2. **Choose an action**: Use an ε-greedy strategy to balance exploration and exploitation.\n",
    "3. **Take the action**: Observe the reward and the next state.\n",
    "4. **Update the Q-table**: Use the Bellman equation to update the Q-values.\n",
    "5. **Repeat**: Continue until the episode ends.\n",
    "\n",
    "Q-Learning involves the agent interacting with the environment and updating a Q-table that contains Q-values for all possible state-action pairs. The agent uses these Q-values to make decisions that maximize its expected cumulative reward. The Q-value for a state-action pair (s, a) is updated using the Bellman equation, which incorporates the reward received and the maximum Q-value of the next state.\n",
    "\n",
    "### Detailed Explanation of the Algorithm\n",
    "\n",
    "- **Exploration-Exploitation Strategy**: The agent must balance exploration (trying new actions to discover their effects) and exploitation (choosing actions that are known to yield high rewards). The ε-greedy strategy is commonly used, where the agent chooses a random action with probability ε, and the best-known action with probability 1-ε.\n",
    "- **Q-Table Initialization**: The Q-table is initialized with arbitrary values, often zeros. It has dimensions `[number of states, number of actions]`.\n",
    "- **State and Action**: The agent starts in an initial state and chooses an action based on the current Q-values and the exploration-exploitation strategy.\n",
    "- **Reward and Next State**: After taking an action, the agent receives a reward and observes the next state.\n",
    "- **Q-Value Update**: The Q-value for the state-action pair is updated using the Bellman equation, incorporating the observed reward and the maximum Q-value of the next state.\n",
    "- **Episode Termination**: The episode terminates when the agent reaches a terminal state (e.g., completing a task or reaching a goal).\n",
    "\n",
    "### Bellman Equation\n",
    "\n",
    "The Bellman equation provides a recursive decomposition for solving Markov Decision Processes (MDPs). The Q-value for a state-action pair (s, a) is updated as follows:\n",
    "\n",
    "$$ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right] $$\n",
    "\n",
    "where:\n",
    "- $ Q(s, a) $: Current Q-value.\n",
    "- $ \\alpha $: Learning rate (0 < α ≤ 1). It determines how much new information overrides the old information.\n",
    "- $ r $: Reward received after taking action $ a $ from state $ s $.\n",
    "- $ \\gamma $: Discount factor (0 ≤ γ ≤ 1) which balances the importance of immediate and future rewards. A higher value of γ puts more emphasis on future rewards.\n",
    "- $ s' $: Next state after taking action $ a $.\n",
    "- $ \\max_{a'} Q(s', a') $: Maximum Q-value of the next state $ s' $ over all possible actions $ a' $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698362ad-8e91-45fd-8acc-734dee9a5793",
   "metadata": {},
   "source": [
    "# Q-Learning Implementation for the Taxi Environment\n",
    "\n",
    "In this section, we will implement the Q-learning algorithm for the Taxi environment. We'll go through each step in detail to understand how the agent learns to optimize its actions.\n",
    "\n",
    "### Step 1: Import Libraries and Initialize the Environment\n",
    "\n",
    "First, we need to import the necessary libraries and create the Taxi environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd6dae9-d19b-421a-bcae-d7442a604b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym\n",
    "#!pip install pygame\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "# Create Taxi environment\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956691f-da54-4f1c-9606-55070d22f4a9",
   "metadata": {},
   "source": [
    "### Step 2: Initialize the Q-Table\n",
    "\n",
    "Next, we initialize the Q-table with zeros. The Q-table has dimensions `[number of states, number of actions]`.\n",
    "\n",
    "-   `state_size`: The total number of possible states.\n",
    "-   `action_size`: The total number of possible actions.\n",
    "-   `q_table`: A table to store Q-values for each state-action pair, initialized to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c86e8-59b5-4e97-8c81-9c5202aae116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "state_size = env.observation_space.n  # Total number of states\n",
    "action_size = env.action_space.n      # Total number of actions\n",
    "q_table = np.zeros((state_size, action_size))\n",
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae488f49-58db-41cf-8636-b51fd64601fb",
   "metadata": {},
   "source": [
    "### Step 3: Define Hyperparameters\n",
    "\n",
    "We define the hyperparameters for the Q-learning algorithm.\n",
    "\n",
    "-   `learning_rate (α)`: Determines how much new information overrides old information.\n",
    "-   `discount_rate (γ)`: Balances the importance of immediate and future rewards.\n",
    "-   `epsilon`: The probability of choosing a random action (exploration).\n",
    "-   `decay_rate`: The rate at which `epsilon` decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39640fc-642e-4be1-a834-6f0c990ceb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1  # Alpha, learning rate\n",
    "discount_rate = 0.6  # Gamma, discount factor\n",
    "epsilon = 1.0        # Exploration rate\n",
    "decay_rate = 0.01    # Decay rate for epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109867f-c112-445e-9f58-6f269f560056",
   "metadata": {},
   "source": [
    "### Step 4: Train the Q-Learning Agent\n",
    "\n",
    "We train the agent over a number of episodes. In each episode, the agent interacts with the environment and updates the Q-table based on its experiences.\n",
    "\n",
    "-   `env.reset()`: Resets the environment to the initial state at the beginning of each episode.\n",
    "-   `random.uniform(0, 1) < epsilon`: Determines whether to explore or exploit.\n",
    "-   `env.action_space.sample()`: Selects a random action (exploration).\n",
    "-   `np.argmax(q_table[state])`: Selects the action with the highest Q-value (exploitation).\n",
    "-   `env.step(action)`: Executes the action and returns the next state, reward, and done flag.\n",
    "-   `q_table[state, action]`: Updates the Q-value using the Bellman equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556cf559-b321-4ca0-a24f-c2ef93eb9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variables\n",
    "num_episodes = 20  # Total number of episodes\n",
    "max_steps = 10       # Max steps per episode\n",
    "\n",
    "# Training the agent\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()  # Reset the environment to the initial state\n",
    "    done = False               # Variable to check if the episode is finished\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore: select a random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Exploit: select the action with max Q-value\n",
    "        \n",
    "        next_state, reward, done, truncated, info = env.step(action)  # Take action and observe the result\n",
    "\n",
    "        # Update Q-table\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (reward + discount_rate * np.max(q_table[next_state]) - q_table[state, action])\n",
    "        \n",
    "        state = next_state  # Move to the next state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Decay epsilon\n",
    "    epsilon = np.exp(-decay_rate * episode)\n",
    "\n",
    "print(f\"Training completed over {num_episodes} episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924b43a8-cdf0-4c76-bca2-80e729f9fb1e",
   "metadata": {},
   "source": [
    "## Step 5: Visualize the Agent's Performance\n",
    "\n",
    "Finally, we visualize the agent's performance by rendering the environment.\n",
    "\n",
    "-   Renders the environment to visualize the agent's actions.\n",
    "-   Uses `sleep` to slow down the rendering for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c2a9fd-eed7-405c-bae8-2945dbd263f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "# Watch the trained agent\n",
    "state, info = env.reset()\n",
    "done = False\n",
    "rewards = 0\n",
    "\n",
    "for s in range(max_steps):\n",
    "    clear_output(wait=True)\n",
    "    env.render()  # Render the environment in the human mode\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    rewards += reward\n",
    "    print(f\"Step {s+1}, Total Reward: {rewards}\")\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
